{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.39\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import qdrant_client\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Union, Dict, Optional\n",
    "from utils import format_metadata\n",
    "\n",
    "import llama_index\n",
    "print(llama_index.__version__)\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.schema import Document, ImageDocument, QueryBundle\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.vector_stores import QdrantVectorStore\n",
    "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "DATABASE_DIR = os.path.join(DATA_DIR, \"db\")\n",
    "REFERENCE_DIR = os.path.join(DATA_DIR, \"reference\")\n",
    "CONSUMER_DIR = os.path.join(DATA_DIR, \"consumer\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(DATA_DIR, \"exp_metadata_1000.csv\"))\n",
    "master_metadata = json.load(open(os.path.join(DATA_DIR, \"metadata\", \"master_metadata.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text embeddings generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference images: 2000\n",
      "Number of consumer images: 5000\n"
     ]
    }
   ],
   "source": [
    "TEXT_EMBED_MODEL = \"text-embedding-3-large\"\n",
    "text_embeddings = OpenAIEmbedding(model=TEXT_EMBED_MODEL)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"reference_list_1000.txt\"), \"r\") as f:\n",
    "    reference_images = f.readlines()\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"consumer_list_1000.txt\"), \"r\") as f:\n",
    "    consumer_images = f.readlines()\n",
    "\n",
    "reference_images = [image.strip() for image in reference_images]\n",
    "consumer_images = [image.strip() for image in consumer_images]\n",
    "\n",
    "print(\"Number of reference images:\", len(reference_images))\n",
    "print(\"Number of consumer images:\", len(consumer_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text embeddings from given metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_metadatas = [format_metadata(metadata) for metadata in metadata_df[\"metadata\"]]\n",
    "# print(formatted_metadatas[0])\n",
    "\n",
    "# text_embs = text_embeddings.get_text_embedding_batch(formatted_metadatas, show_progress=True)\n",
    "\n",
    "# text_emb_dict = {}\n",
    "# for ndc11, image_file, text_emb, metadata, text_content in zip(metadata_df[\"ndc11\"], metadata_df[\"first_reference\"], text_embs, metadata_df[\"metadata\"], formatted_metadatas):\n",
    "#     name = master_metadata[image_file]['name']\n",
    "#     text_emb_dict[ndc11] = {\n",
    "#         \"name\": name, \"text_emb\": text_emb, \"metadata\": metadata, \"text_content\": text_content\n",
    "#     }\n",
    "\n",
    "# with open(os.path.join(DATA_DIR, \"embeddings\", f\"REFERENCE_EXTRADISP_{TEXT_EMBED_MODEL}.json\"), \"w\") as f:\n",
    "#     json.dump(text_emb_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Embeddings from GPT-4V descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"reference_1-1000/reference-gpt-extracted-features-n=1000.json\"), \"r\") as f:\n",
    "    ref_emb_text_dicts = json.load(f)\n",
    "    \n",
    "for k, v in ref_emb_text_dicts.items():\n",
    "    if not v:\n",
    "        ref_emb_text_dicts[k] = format_metadata(master_metadata[k][\"metadata\"])\n",
    "\n",
    "reference_emb_texts = [ref_emb_text_dicts[ref_image] for ref_image in reference_images] \n",
    "reference_metadatas = [master_metadata[ref_image] for ref_image in reference_images]\n",
    "\n",
    "# reference_text_embs = text_embeddings.get_text_embedding_batch(reference_emb_texts, show_progress=True)\n",
    "\n",
    "# reference_text_emb_dict = {}\n",
    "\n",
    "# for reference_image, reference_emb_text, reference_metadata, reference_text_emb \\\n",
    "#     in zip(reference_images, reference_emb_texts, reference_metadatas, reference_text_embs):\n",
    "#         reference_text_emb_dict[reference_image] = {\n",
    "#             'name': reference_metadata[\"name\"],\n",
    "#             'text_emb': reference_text_emb,\n",
    "#             'metadata': reference_metadata,\n",
    "#             'text_content': reference_emb_text\n",
    "#         }\n",
    "\n",
    "# with open(os.path.join(DATA_DIR, \"embeddings\", f\"REFERENCE_GPTV_{TEXT_EMBED_MODEL}.json\"), \"w\") as f:\n",
    "#     json.dump(reference_text_emb_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid extracted descriptions: 4895\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"consumer_1-1000/consumer-gpt-extracted-features-n=1000.json\"), \"r\") as f:\n",
    "    consumer_emb_text_dicts = json.load(f)\n",
    "\n",
    "consumer_emb_texts = [consumer_emb_text_dicts[image] for image in consumer_images] \n",
    "consumer_metadatas = [master_metadata[image] for image in consumer_images]\n",
    "\n",
    "valid_indices = [idx for idx, content in enumerate(consumer_emb_texts) if content]\n",
    "print(\"Number of valid extracted descriptions:\", len(valid_indices))\n",
    "\n",
    "valid_consumer_emb_texts = [consumer_emb_texts[idx] for idx in valid_indices]\n",
    "\n",
    "# consumer_text_embs = text_embeddings.get_text_embedding_batch(valid_consumer_emb_texts, show_progress=True)\n",
    "\n",
    "consumer_text_emb_dict = {}\n",
    "\n",
    "for consumer_image, consumer_metadata \\\n",
    "    in zip(consumer_images, consumer_metadatas):\n",
    "        consumer_text_emb_dict[consumer_image] = {\n",
    "            'name': consumer_metadata[\"name\"], 'text_emb': None,\n",
    "            'metadata': consumer_metadata, 'text_content': None\n",
    "        }\n",
    "\n",
    "for valid_idx, valid_consumer_emb_text, consumer_text_emb in zip(valid_indices, valid_consumer_emb_texts, consumer_text_embs):\n",
    "    consumer_image = consumer_images[valid_idx]\n",
    "    consumer_text_emb_dict[consumer_image]['text_emb'] = consumer_text_emb\n",
    "    consumer_text_emb_dict[consumer_image]['text_content'] = valid_consumer_emb_text\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"embeddings\", f\"CONSUMER_GPTV_{TEXT_EMBED_MODEL}.json\"), \"w\") as f:\n",
    "    json.dump(consumer_text_emb_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"embeddings\", f\"CONSUMER_GPTV_{TEXT_EMBED_MODEL}.json\"), \"w\") as f:\n",
    "    json.dump(consumer_text_emb_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(DATA_DIR, \"embeddings\", \"REFERENCE_EXTRADISP_text-embedding-3-large.json\"), \"r\") as f:\n",
    "#     text_emb_dict = json.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"embeddings\", \"REFERENCE_GPTV_text-embedding-3-large.json\"), \"r\") as f:\n",
    "    text_emb_dict = json.load(f)\n",
    "    \n",
    "reference_text_embs = [text_emb_dict[ref_image]['text_emb'] for ref_image in reference_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(path = os.path.join(DATABASE_DIR, \"qdrant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "text_document_list = []\n",
    "for reference_image, reference_emb_text, reference_metadata, reference_text_emb \\\n",
    "    in zip(reference_images, reference_emb_texts, reference_metadatas, reference_text_embs):\n",
    "    reference_metadata.update({\"image_path\": reference_image})\n",
    "    text_document_list.append(\n",
    "        Document(\n",
    "            text=reference_emb_text,\n",
    "            metadata=reference_metadata,\n",
    "            embedding=reference_text_emb\n",
    "            )\n",
    "    )\n",
    "    \n",
    "print(len(text_document_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_store = QdrantVectorStore(client=client, collection_name=\"text_only\")\n",
    "text_service_context = ServiceContext.from_defaults(embed_model = text_embeddings)\n",
    "text_storage_context = StorageContext.from_defaults(vector_store = text_store)\n",
    "\n",
    "text_index = VectorStoreIndex.from_documents(\n",
    "    documents = text_document_list,\n",
    "    storage_context=text_storage_context,\n",
    "    service_context=text_service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = text_index.as_retriever(similarity_top_k=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Embeddings Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"embeddings\", \"clip-ViT-L14@336px-image-all.json\"), \"r\") as f:\n",
    "    image_emb_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_document_list = []\n",
    "for reference_image, reference_emb_text, reference_metadata, reference_text_emb \\\n",
    "    in zip(reference_images, reference_emb_texts, reference_metadatas, reference_text_embs):\n",
    "    reference_metadata.update({\"image_path\": reference_image})\n",
    "    image_document_list.append(\n",
    "        ImageDocument(\n",
    "            image_path=reference_image,\n",
    "            text=reference_emb_text,\n",
    "            metadata=reference_metadata,\n",
    "            embedding=image_emb_dict[reference_image],\n",
    "            text_embedding=reference_text_emb\n",
    "            )\n",
    "    )\n",
    "    \n",
    "print(len(image_document_list))\n",
    "len(image_document_list[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_store = QdrantVectorStore(client=client, collection_name=\"image_only\")\n",
    "image_service_context = ServiceContext.from_defaults(embed_model = text_embeddings)\n",
    "image_storage_context = StorageContext.from_defaults(image_store = image_store)\n",
    "\n",
    "image_index = VectorStoreIndex.from_documents(\n",
    "    documents = image_document_list,\n",
    "    storage_context=image_storage_context,\n",
    "    service_context=image_service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_retriever = image_index.as_retriever(similarity_top_k = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [20:35<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "\n",
    "for input_query in tqdm(reference_images):\n",
    "    input_query_bundle = QueryBundle(\n",
    "        query_str = \"dummy\",\n",
    "        embedding = image_emb_dict[input_query]\n",
    "    )\n",
    "\n",
    "    retrieved_images = image_retriever.retrieve(input_query_bundle)\n",
    "    \n",
    "    for idx, retrieved_image in enumerate(retrieved_images):\n",
    "        if retrieved_image.node.metadata[\"ndc11\"] == master_metadata[input_query][\"ndc11\"]:\n",
    "            if idx != 0:\n",
    "                rank += (idx+1)\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Text and Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_document_list = []\n",
    "for reference_image, reference_emb_text, reference_metadata, reference_text_emb \\\n",
    "    in zip(reference_images, reference_emb_texts, reference_metadatas, reference_text_embs):\n",
    "    reference_metadata.update({\"image_path\": reference_image})\n",
    "    image_document_list.append(\n",
    "        ImageDocument(\n",
    "            image_path=reference_image,\n",
    "            text=reference_emb_text,\n",
    "            metadata=reference_metadata,\n",
    "            embedding=image_emb_dict[reference_image],\n",
    "            text_embedding=reference_text_emb\n",
    "            )\n",
    "    )\n",
    "    \n",
    "print(len(image_document_list))\n",
    "len(image_document_list[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_document_list = []\n",
    "for reference_image, reference_emb_text, reference_metadata, reference_text_emb \\\n",
    "    in zip(reference_images, reference_emb_texts, reference_metadatas, reference_text_embs):\n",
    "    reference_metadata.update({\"image_path\": reference_image})\n",
    "    image_document_list.append(\n",
    "        ImageDocument(\n",
    "            image_path=reference_image,\n",
    "            text=reference_emb_text,\n",
    "            metadata=reference_metadata,\n",
    "            embedding=image_emb_dict[reference_image],\n",
    "            text_embedding=reference_text_emb\n",
    "            )\n",
    "    )\n",
    "    \n",
    "print(len(image_document_list))\n",
    "len(image_document_list[0].embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
